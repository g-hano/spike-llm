{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d7d3336",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "files = [f\"D:/tokenized/temp_chunk_{i:04d}.parquet\" for i in range(77)]\n",
    "dataset = load_dataset(\"parquet\", data_files=files, streaming=True)\n",
    "first_row = next(iter(dataset[\"train\"]))\n",
    "print(first_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3ca239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import math\n",
    "from model.transformer import SpikingLLM, SpikingMoELLM, RegularLLM\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65bd0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTokenizedDataset(IterableDataset):\n",
    "    def __init__(self, data_dir, seq_len=1024, vocab_size=65536, chunks_start=0, chunks_end=77):\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        files = [f\"{data_dir}/temp_chunk_{i:04d}.parquet\" for i in range(chunks_start, chunks_end)]\n",
    "        self.dataset = load_dataset(\"parquet\", data_files=files, streaming=True)[\"train\"]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for item in self.dataset:\n",
    "            tokens = torch.tensor(item['input_ids'], dtype=torch.long)\n",
    "            \n",
    "            if len(tokens) < self.seq_len + 1:\n",
    "                padding = torch.zeros(self.seq_len + 1 - len(tokens), dtype=torch.long)\n",
    "                tokens = torch.cat([tokens, padding])\n",
    "            else:\n",
    "                tokens = tokens[:self.seq_len + 1]\n",
    "            \n",
    "            input_ids = tokens[:-1]\n",
    "            labels = tokens[1:]\n",
    "            \n",
    "            yield {\n",
    "                'input_ids': input_ids,\n",
    "                'labels': labels\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9caf0f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_configs():\n",
    "    return {\n",
    "        \"35m\": {\n",
    "            \"vocab_size\": 65536,\n",
    "            \"d_model\": 256,\n",
    "            \"n_heads\": 4,\n",
    "            \"n_kv_heads\": 2,\n",
    "            \"num_layers\": 2,\n",
    "            \"intermediate_size\": 512,\n",
    "            \"max_seq_len\": 2048,\n",
    "            \"rope_theta_local\": 10_000,\n",
    "            \"rope_theta_global\": 1_000_000,\n",
    "            \"window_size\": 256,\n",
    "            \"dropout\": 0.0,\n",
    "            \"beta\": 0.95,\"num_steps\":5,\n",
    "        },\n",
    "        \"170m\": {\n",
    "            \"vocab_size\": 65536,\n",
    "            \"d_model\": 512,\n",
    "            \"n_heads\": 8*4,\n",
    "            \"n_kv_heads\": 4*4,\n",
    "            \"num_layers\": 8*4,\n",
    "            \"intermediate_size\": 1536,\n",
    "            \"max_seq_len\": 4096,\n",
    "            \"rope_theta_local\": 10_000,\n",
    "            \"rope_theta_global\": 1_000_000,\n",
    "            \"window_size\": 512,\n",
    "            \"dropout\": 0.0,\n",
    "            \"beta\": 0.95,\"num_steps\":5,\n",
    "        },\n",
    "        \"350m\": {\n",
    "            \"vocab_size\": 65536,\n",
    "            \"d_model\": 768,\n",
    "            \"n_heads\": 32,\n",
    "            \"n_kv_heads\": 16,\n",
    "            \"num_layers\": 32,\n",
    "            \"intermediate_size\": 2560,\n",
    "            \"max_seq_len\": 8192,\n",
    "            \"rope_theta_local\": 10_000,\n",
    "            \"rope_theta_global\": 1_000_000,\n",
    "            \"window_size\": 1024,\n",
    "            \"dropout\": 0.0,\n",
    "            \"beta\": 0.95,\"num_steps\":5,\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8716107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_schedule(step, warmup_steps, max_steps, max_lr, min_lr):\n",
    "    if max_steps is None:\n",
    "        return max_lr if step >= warmup_steps else max_lr * step / warmup_steps\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * step / warmup_steps\n",
    "    if step > max_steps:\n",
    "        return min_lr\n",
    "    decay_ratio = (step - warmup_steps) / (max_steps - warmup_steps)\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (max_lr - min_lr)\n",
    "\n",
    "def collect_spike_activity(model):\n",
    "    spike_rates = {}\n",
    "    total_spikes = 0\n",
    "    total_neurons = 0\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if hasattr(module, 'spike_mem') and module.spike_mem is not None:\n",
    "            if hasattr(module, 'spike'):\n",
    "                spike_rate = torch.mean(module.spike_mem.float()).item()\n",
    "                spike_rates[f\"spike_rate/{name}\"] = spike_rate\n",
    "                total_spikes += torch.sum(module.spike_mem).item()\n",
    "                total_neurons += module.spike_mem.numel()\n",
    "    \n",
    "    if total_neurons > 0:\n",
    "        spike_rates[\"spike_rate/overall\"] = total_spikes / total_neurons\n",
    "    \n",
    "    return spike_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47cbb7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_plots():\n",
    "    plt.ion()\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def update_plots(ax1, ax2, steps, losses, spike_rates, output_dir):\n",
    "    ax1.clear()\n",
    "    ax1.plot(steps, losses, 'b-', alpha=0.7)\n",
    "    ax1.set_xlabel('Steps')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Training Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.clear()\n",
    "    if spike_rates:\n",
    "        ax2.plot(steps, spike_rates, 'r-', alpha=0.7)\n",
    "        ax2.set_xlabel('Steps')\n",
    "        ax2.set_ylabel('Spike Rate')\n",
    "        ax2.set_title('Overall Spike Activity')\n",
    "        ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_progress.png'), dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f118dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, gradient_accumulation_steps, max_grad_norm):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training\")):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits, _ = model(input_ids)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59b84c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits, _ = model(input_ids)\n",
    "                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a096b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SIZE = '170m'\n",
    "USE_MOE = False\n",
    "MOE_RATIO = 0.25\n",
    "DATA_DIR = 'D:/tokenized'\n",
    "TOKENIZER_PATH = 'D:/fineweb-chunked/clean-bpe-tokenizer'\n",
    "OUTPUT_DIR = './checkpoints'\n",
    "SEQ_LEN = 1024\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_STEPS = 2\n",
    "LR = 1e-4\n",
    "W_DECAY = 0.01\n",
    "MAX_GRAD_NORM = 1.0\n",
    "NUM_EPOCHS = 1\n",
    "WARMUP_STEPS = 2_000 #8_000\n",
    "MAX_STEPS = None\n",
    "EVAL_INTERVAL = 1_000\n",
    "SAVE_INTERVAL = 5_000\n",
    "RESUME_FROM = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0aa2f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "\ttorch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "\tdevice = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Running on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dee86093",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "422233e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_model_configs()[MODEL_SIZE]\n",
    "config[\"max_seq_len\"] = max(config[\"max_seq_len\"], SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d63c44b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dense model\n"
     ]
    }
   ],
   "source": [
    "if USE_MOE:\n",
    "\tnum_moe_layers = max(1, int(config[\"num_layers\"] * MOE_RATIO))\n",
    "\tstep = config[\"num_layers\"] // num_moe_layers\n",
    "\tmoe_layers = [i * step + step - 1 for i in range(num_moe_layers)]\n",
    "\tconfig.update({\n",
    "\t\t\"moe_layers\": moe_layers,\n",
    "\t\t\"num_experts\": 8,\n",
    "\t\t\"num_active\": 2,\n",
    "\t\t\"tie_embeddings\": True,\n",
    "\t\t\"embedding_dropout\": 0.0,\n",
    "\t})\n",
    "\tmodel = SpikingMoELLM(**config).to(device)\n",
    "\tprint(\"Loaded MoE model\")\n",
    "else:\n",
    "\t#model = RegularLLM(**config).to(device)\n",
    "\tmodel = SpikingLLM(**config).to(device)\n",
    "\tprint(\"Loaded Dense model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05afb9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 167,920,320\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model parameters: {model.get_num_params():,}\")\n",
    "if USE_MOE and hasattr(model, 'count_moe_params'):\n",
    "\tmoe_params, regular_params = model.count_moe_params()\n",
    "\tactive_params = regular_params + moe_params * (config[\"num_active\"] / config[\"num_experts\"])\n",
    "\tprint(f\"Active parameters: {active_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d3e80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(TOKENIZER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "492252d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25ec58d5dbc43508bdc76d58fdac968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = PreTokenizedDataset(DATA_DIR, SEQ_LEN, config[\"vocab_size\"], 0, 70)\n",
    "eval_dataset = PreTokenizedDataset(DATA_DIR, SEQ_LEN, config[\"vocab_size\"], 70, 77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e02a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "\tdataset, \n",
    "\tbatch_size=BATCH_SIZE, \n",
    "\tnum_workers=0,\n",
    "\tpin_memory=True\n",
    ")\n",
    "eval_loader = DataLoader(\n",
    "\teval_dataset, \n",
    "\tbatch_size=BATCH_SIZE, \n",
    "\tnum_workers=0,\n",
    "\tpin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfddf931",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "\tmodel.parameters(),\n",
    "\tlr=LR,\n",
    "\tweight_decay=W_DECAY,\n",
    "\tbetas=(0.9, 0.95)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddc7a668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax1, ax2 = setup_plots()\n",
    "train_losses = []\n",
    "train_steps = []\n",
    "spike_rates_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28588a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_step = 0\n",
    "if RESUME_FROM:\n",
    "\tcheckpoint = torch.load(RESUME_FROM, map_location=device)\n",
    "\tmodel.load_state_dict(checkpoint['model'])\n",
    "\toptimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\tstart_step = checkpoint['step']\n",
    "\tprint(f\"Resumed from step {start_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cd75eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 1046it [26:43,  1.53s/it, loss=10.9964, lr=1.00e-04, step=1000]C:\\Users\\Cihan\\AppData\\Local\\Temp\\ipykernel_32\\129283327.py:11: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Evaluating: 10773it [1:50:20,  1.63it/s]\n",
      "Epoch 1/1: 1000it [2:17:04,  8.22s/it, loss=10.9964, lr=1.00e-04, step=1000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m \tupdate_plots(ax1, ax2, train_steps, train_losses, spike_rates_history, OUTPUT_DIR)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m EVAL_INTERVAL \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m \teval_loss \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \tperplexity \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(eval_loss)\n\u001b[0;32m     52\u001b[0m \tpbar\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: eval_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, perplexity=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mperplexity\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, dataloader, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[1;32m---> 12\u001b[0m     logits, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     15\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\project\\src\\model\\transformer.py:180\u001b[0m, in \u001b[0;36mSpikingLLM.forward\u001b[1;34m(self, input_ids, use_cache, past_key_values)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m    179\u001b[0m     past_kv \u001b[38;5;241m=\u001b[39m past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     x, present_kv \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m     presents\u001b[38;5;241m.\u001b[39mappend(present_kv)\n\u001b[0;32m    183\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_norm(x)\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\project\\src\\model\\transformer.py:125\u001b[0m, in \u001b[0;36mSpikingTransformerBlock.forward\u001b[1;34m(self, x, use_cache, past_key_value, layer_idx)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, use_cache\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, past_key_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, layer_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Attention\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m--> 125\u001b[0m     attn_out, present_kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_gate \u001b[38;5;241m*\u001b[39m attn_out\n\u001b[0;32m    128\u001b[0m     x_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\project\\src\\model\\layers.py:261\u001b[0m, in \u001b[0;36mSpikingGroupedSlidingAttention.forward\u001b[1;34m(self, x, use_cache, past_key_value, layer_idx)\u001b[0m\n\u001b[0;32m    258\u001b[0m spike_acc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(out)\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps):\n\u001b[1;32m--> 261\u001b[0m     spk, mem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m     spike_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m spk\n\u001b[0;32m    264\u001b[0m output \u001b[38;5;241m=\u001b[39m (spike_acc \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps) \u001b[38;5;241m*\u001b[39m out\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspike_scale\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\snntorch\\_neurons\\leaky.py:209\u001b[0m, in \u001b[0;36mLeaky.forward\u001b[1;34m(self, input_, mem)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m input_\u001b[38;5;241m.\u001b[39mshape:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(input_, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmem \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_function(input_)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_quant:\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\snntorch\\_neurons\\neurons.py:106\u001b[0m, in \u001b[0;36mSpikingNeuron.mem_reset\u001b[1;34m(self, mem)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generates detached reset signal if mem > threshold.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03mReturns reset.\"\"\"\u001b[39;00m\n\u001b[0;32m    105\u001b[0m mem_shift \u001b[38;5;241m=\u001b[39m mem \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\n\u001b[1;32m--> 106\u001b[0m reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspike_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmem_shift\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reset\n",
      "File \u001b[1;32mc:\\Users\\Cihan\\Desktop\\llamaindex\\lindex\\Lib\\site-packages\\snntorch\\surrogate.py:210\u001b[0m, in \u001b[0;36matan.<locals>.inner\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"ArcTan surrogate gradient enclosed with a parameterized slope.\"\"\"\u001b[39;00m\n\u001b[0;32m    208\u001b[0m alpha \u001b[38;5;241m=\u001b[39m alpha\n\u001b[1;32m--> 210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(x):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ATan\u001b[38;5;241m.\u001b[39mapply(x, alpha)\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = start_step\n",
    "\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\tpbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\t\n",
    "\tfor batch in pbar:\n",
    "\t\tif MAX_STEPS and step >= MAX_STEPS:\n",
    "\t\t\tbreak\n",
    "\t\t\n",
    "\t\tlr = LR #get_lr_schedule(step, WARMUP_STEPS, MAX_STEPS, LR, MIN_LR)\n",
    "\t\tfor param_group in optimizer.param_groups:\n",
    "\t\t\tparam_group['lr'] = lr\n",
    "\t\t\n",
    "\t\tinput_ids = batch['input_ids'].to(device)\n",
    "\t\tlabels = batch['labels'].to(device)\n",
    "\t\t\n",
    "\t\twith torch.amp.autocast('cuda'):\n",
    "\t\t\tlogits, _ = model(input_ids)\n",
    "\t\t\tloss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\t\t\tloss = loss / GRADIENT_STEPS\n",
    "\t\t\n",
    "\t\tloss.backward()\n",
    "\n",
    "\t\tif (step + 1) % GRADIENT_STEPS == 0:\n",
    "\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\tif step % 10 == 0:\n",
    "\t\t\t\tmodel.reset_mem()\n",
    "\t\t\n",
    "\t\tspike_activity = collect_spike_activity(model)\n",
    "\t\tcurrent_loss = loss.item() * GRADIENT_STEPS\n",
    "\n",
    "\t\ttrain_losses.append(current_loss)\n",
    "\t\ttrain_steps.append(step)\n",
    "\t\tspike_rates_history.append(spike_activity.get(\"spike_rate/overall\", 0))\n",
    "\t\t\n",
    "\t\tpbar.set_postfix({'loss': f'{current_loss:.4f}', 'lr': f'{lr:.2e}', 'step': step})\n",
    "\t\tpbar.update(1)\n",
    "\n",
    "\t\tif step % 50 == 0:\n",
    "\t\t\tupdate_plots(ax1, ax2, train_steps, train_losses, spike_rates_history, OUTPUT_DIR)\n",
    "\t\t\n",
    "\t\tif step % EVAL_INTERVAL == 0 and step > 0:\n",
    "\t\t\teval_loss = evaluate(model, eval_loader, device)\n",
    "\t\t\tperplexity = math.exp(eval_loss)\n",
    "\t\t\t\n",
    "\t\t\tpbar.write(f\"Step {step}: eval_loss={eval_loss:.4f}, perplexity={perplexity:.2f}\")\n",
    "\t\t\tmodel.train()\n",
    "\t\t\n",
    "\t\tif step % SAVE_INTERVAL == 0 and step > 0:\n",
    "\t\t\tcheckpoint = {\n",
    "\t\t\t\t'model': model.state_dict(),\n",
    "\t\t\t\t'optimizer': optimizer.state_dict(),\n",
    "\t\t\t\t'step': step,\n",
    "\t\t\t\t'epoch': epoch,\n",
    "\t\t\t\t'config': config\n",
    "\t\t\t}\n",
    "\t\t\t\n",
    "\t\t\tcheckpoint_path = os.path.join(OUTPUT_DIR, f\"checkpoint_step_{step}.pt\")\n",
    "\t\t\ttorch.save(checkpoint, checkpoint_path)\n",
    "\t\t\tpbar.write(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\t\t\n",
    "\t\tstep += 1\n",
    "\t\t\n",
    "pbar.close()\n",
    "final_checkpoint = {\n",
    "\t'model': model.state_dict(),\n",
    "\t'optimizer': optimizer.state_dict(),\n",
    "\t'step': step,\n",
    "\t'epoch': epoch,\n",
    "\t'config': config,\n",
    "}\n",
    "\n",
    "final_path = os.path.join(OUTPUT_DIR, f\"final_model.pt\")\n",
    "torch.save(final_checkpoint, final_path)\n",
    "print(f\"Training completed. Final model saved to {final_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e845156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training state\n",
    "epoch = 0\n",
    "step = 0\n",
    "best_val_loss = float('inf')\n",
    "start_time = time.time()\n",
    "\n",
    "# Resume from checkpoint if specified\n",
    "if RESUME_FROM and os.path.exists(RESUME_FROM):\n",
    "    print(f\"Resuming from {RESUME_FROM}\")\n",
    "    checkpoint = torch.load(RESUME_FROM, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint.get('epoch', 0)\n",
    "    step = checkpoint.get('step', 0)\n",
    "    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n",
    "    print(f\"Resumed from epoch {epoch}, step {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59e4d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/1 ===\n",
      "Learning rate: 0.00e+00\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 0it [00:00, ?it/s]C:\\Users\\Cihan\\AppData\\Local\\Temp\\ipykernel_24632\\1224566086.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Training: 3083it [1:17:23,  1.45s/it]"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for current_epoch in range(epoch, NUM_EPOCHS):\n",
    "    print(f\"\\n=== Epoch {current_epoch + 1}/{NUM_EPOCHS} ===\")\n",
    "    \n",
    "    # Update learning rate for this epoch\n",
    "    current_lr = get_lr_schedule(step, WARMUP_STEPS, MAX_STEPS, LR, LR * 0.1)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = current_lr\n",
    "    \n",
    "    print(f\"Learning rate: {current_lr:.2e}\")\n",
    "    \n",
    "    # Training epoch\n",
    "    print(\"Training...\")\n",
    "    train_loss = train_epoch(\n",
    "        model=model,\n",
    "        dataloader=train_loader, \n",
    "        optimizer=optimizer,\n",
    "        device=device,\n",
    "        gradient_accumulation_steps=GRADIENT_STEPS,\n",
    "        max_grad_norm=MAX_GRAD_NORM\n",
    "    )\n",
    "    \n",
    "    # Collect spike activity after training\n",
    "    spike_activity = collect_spike_activity(model)\n",
    "    overall_spike_rate = spike_activity.get(\"spike_rate/overall\", 0.0)\n",
    "    \n",
    "    # Update metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_steps.append(current_epoch)\n",
    "    spike_rates_history.append(overall_spike_rate)\n",
    "    \n",
    "    print(f\"Epoch {current_epoch + 1} - Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Spike Rate: {overall_spike_rate:.4f}\")\n",
    "    \n",
    "    # Evaluation\n",
    "    if (current_epoch + 1) % (EVAL_INTERVAL // len(train_loader)) == 0 or current_epoch == NUM_EPOCHS - 1:\n",
    "        print(\"Evaluating...\")\n",
    "        val_loss = evaluate(model, eval_loader, device)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Update plots\n",
    "        update_plots(ax1, ax2, train_steps, train_losses, spike_rates_history, OUTPUT_DIR)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_checkpoint = {\n",
    "                'epoch': current_epoch,\n",
    "                'step': step,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'config': config,\n",
    "                'model_size': MODEL_SIZE,\n",
    "                'spike_rates': spike_activity\n",
    "            }\n",
    "            torch.save(best_checkpoint, os.path.join(OUTPUT_DIR, f'best_model_{MODEL_SIZE}.pt'))\n",
    "            print(f\"✓ New best model saved (val_loss: {val_loss:.4f})\")\n",
    "        \n",
    "        # Log all spike rates\n",
    "        if spike_activity:\n",
    "            print(\"Spike rates by layer:\")\n",
    "            for layer_name, rate in spike_activity.items():\n",
    "                if 'spike_rate' in layer_name:\n",
    "                    print(f\"  {layer_name}: {rate:.4f}\")\n",
    "    \n",
    "    # Regular checkpointing\n",
    "    if (current_epoch + 1) % (SAVE_INTERVAL // len(train_loader)) == 0:\n",
    "        checkpoint = {\n",
    "            'epoch': current_epoch,\n",
    "            'step': step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': best_val_loss,\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'config': config,\n",
    "            'model_size': MODEL_SIZE,\n",
    "            'spike_rates': spike_activity\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(OUTPUT_DIR, f'checkpoint_epoch_{current_epoch + 1}.pt'))\n",
    "        print(f\"✓ Checkpoint saved at epoch {current_epoch + 1}\")\n",
    "    \n",
    "    # Update step counter (approximate)\n",
    "    step += len(train_loader) // GRADIENT_STEPS\n",
    "    \n",
    "    # Early stopping conditions\n",
    "    if train_loss > 15.0:\n",
    "        print(f\"⚠️  WARNING: Training loss explosion ({train_loss:.4f}). Consider reducing learning rate.\")\n",
    "    \n",
    "    if train_loss < 0.1:\n",
    "        print(f\"✓ Training loss very low ({train_loss:.4f}). Model may be converging.\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b52b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final results\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n=== Training Complete ===\")\n",
    "print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final spike rate: {spike_rates_history[-1]:.4f}\")\n",
    "\n",
    "# Save final model\n",
    "final_checkpoint = {\n",
    "    'epoch': NUM_EPOCHS,\n",
    "    'step': step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'train_loss': train_losses[-1] if train_losses else float('inf'),\n",
    "    'val_loss': best_val_loss,\n",
    "    'best_val_loss': best_val_loss,\n",
    "    'config': config,\n",
    "    'model_size': MODEL_SIZE,\n",
    "    'training_time': total_time,\n",
    "    'final_spike_rates': spike_activity\n",
    "}\n",
    "torch.save(final_checkpoint, os.path.join(OUTPUT_DIR, f'final_model_{MODEL_SIZE}.pt'))\n",
    "print(f\"✓ Final model saved\")\n",
    "\n",
    "# Final plot update\n",
    "update_plots(ax1, ax2, train_steps, train_losses, spike_rates_history, OUTPUT_DIR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db6446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2637db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lindex (3.11.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
